{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallel computing with MPI-3 RMA and Julia\n",
    "\n",
    "<br/>\n",
    "### JuliaCon 2018\n",
    "<br/>\n",
    "[Bart Janssens](https://github.com/barche)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bird's eye view\n",
    "\n",
    "1. Motivation\n",
    "1. Classic MPI message passing\n",
    "2. Native Julia parallelism\n",
    "3. MPI-3 one-sided \"Remote Memory Access\" (RMA)\n",
    "4. Applications:\n",
    "    - MPI Arrays\n",
    "    - Julia IO on MPI\n",
    "    - New clustermanager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "Julia already has multicore processing support, so why MPI?\n",
    "\n",
    "* The standard communication library on scientific clusters\n",
    "* Interfacing with legacy Fortran/C/C++ code\n",
    "* Optimal MPI implementations exist for specific interconnects (e.g. InfiniBand)\n",
    "* Thanks to `ccall`, integration is cheap and easy and MPI can be made more friendly to Julia programmers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The MPI way\n",
    "* MPI: Message passing interface\n",
    "* Original: **cooperative** communication\n",
    "\n",
    "|Rank 0|Rank 1|\n",
    "|---|---|---|\n",
    "|Init()|Init()|\n",
    "|mymessage = \"Hi 1!\"||\n",
    "|Send(mymessage,1)|Recv(buffer)|\n",
    "|Finalize()|Finalize()|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The same program is sent to all ranks:\n",
    "```julia\n",
    "using MPI\n",
    "\n",
    "MPI.Init()\n",
    "\n",
    "const comm = MPI.COMM_WORLD\n",
    "const rank = MPI.Comm_rank(comm)\n",
    "const tag = 0\n",
    "\n",
    "if rank == 0\n",
    "  msg = unsafe_wrap(Vector{UInt8},\"Hi 1!\")\n",
    "  println(\"Sending message from rank 0 to rank 1\")\n",
    "  MPI.Send(msg, 1, tag, comm)\n",
    "elseif rank == 1\n",
    "  rec_buf = fill(UInt8('_'), 10)\n",
    "  MPI.Recv!(rec_buf, 0, tag, comm)\n",
    "  println(\"Received message $(String(rec_buf)) from rank 0\")\n",
    "end\n",
    "\n",
    "println(\"Rank $rank is finalizing.\")\n",
    "MPI.Finalize()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Output of `mpirun -np 4 julia --project src/sendrecv.jl`:\n",
    "\n",
    "```text\n",
    "Sending message from rank 0 to rank 1\n",
    "Rank 3 is finalizing.\n",
    "Rank 2 is finalizing.\n",
    "Received message Hi 1!_____ from rank 0\n",
    "Rank 0 is finalizing.\n",
    "Rank 1 is finalizing.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Collective calls\n",
    "* Send/Receive works only between two processes\n",
    "* Collective calls exist, e.g. summing an array:\n",
    "\n",
    "```julia\n",
    "using MPI\n",
    "\n",
    "MPI.Init()\n",
    "\n",
    "const comm = MPI.COMM_WORLD\n",
    "const rank = MPI.Comm_rank(comm)\n",
    "const root = 0\n",
    "\n",
    "vec = fill(1,10)\n",
    "s = MPI.Reduce(sum(vec), +, root, comm) # MUST be called on all ranks\n",
    "println(\"Sum on rank $rank: $(repr(s))\")\n",
    "\n",
    "MPI.Finalize()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Output of `mpirun -np 4 julia --project src/reduce.jl`:\n",
    "\n",
    "```text\n",
    "Sum on rank 2: nothing\n",
    "Sum on rank 3: nothing\n",
    "Sum on rank 0: 40\n",
    "Sum on rank 1: nothing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or using `Allreduce`:\n",
    "```text\n",
    "Sum on rank 2: 40Sum on rank 3: 40\n",
    "Sum on rank 1: 40\n",
    "\n",
    "Sum on rank 0: 40\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Julia way\n",
    "* One-sided\n",
    "* Remote references and remote call\n",
    "* Directed by a \"master\" process.\n",
    "* \"Workers\" do the actual work and can be started and stopped using `addprocs` and `rmprocs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Starting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Distributed\n",
    "addprocs(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Creating some arrays remotely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Future,1}:\n",
       " Future(2, 1, 102, nothing)\n",
       " Future(3, 1, 103, nothing)\n",
       " Future(4, 1, 104, nothing)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_arrays = [@spawnat w fill(myid(),10) for w in workers()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fetching the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Int64,1}:\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch(remote_arrays[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compute the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 3:\tComputing sum for Future(3, 1, 103, nothing) on process 3\n",
      "      From worker 4:\tComputing sum for Future(4, 1, 104, nothing) on process 4\n",
      "      From worker 2:\tComputing sum for Future(2, 1, 102, Some([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])) on process 2\n"
     ]
    }
   ],
   "source": [
    "sums = map(remote_arrays) do x\n",
    "    @spawn begin\n",
    "        println(\"Computing sum for $x on process $(myid())\")\n",
    "        return sum(fetch(x))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(fetch.(sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Future,1}:\n",
       " Future(2, 1, 106, Some(20))\n",
       " Future(3, 1, 107, Some(30))\n",
       " Future(4, 1, 108, Some(40))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or using the higher level `@distributed` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 2:\tComputing sum for Future(2, 1, 102, Some([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])) on process 2\n",
      "      From worker 3:\tComputing sum for Future(3, 1, 103, Some([3, 3, 3, 3, 3, 3, 3, 3, 3, 3])) on process 3\n",
      "      From worker 4:\tComputing sum for Future(4, 1, 104, Some([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])) on process 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@distributed (+) for x=remote_arrays\n",
    "    println(\"Computing sum for $x on process $(myid())\")\n",
    "    sum(fetch(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MPI one-sided communication\n",
    "* First introduced in MPI-2, extended in MPI-3\n",
    "* Focus here on \"Remote Memory Access\" (RMA)\n",
    "* Basic idea: define a region of memory on each process, that other processes can read from or write to\n",
    "* Target can be passive\n",
    "* Needs locking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Setup of the shared memory region\n",
    "\n",
    "```julia\n",
    "# set up an array of length N on each process and mark it shared\n",
    "shared = zeros(N)\n",
    "win = MPI.Win()\n",
    "MPI.Win_create(shared, MPI.INFO_NULL, comm, win)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Have other ranks write the value of their rank to rank 0\n",
    "\n",
    "```julia\n",
    "# Every rank writes to the array held by rank 0 (dest) at its rank position\n",
    "offset = rank\n",
    "nb_elms = 1\n",
    "MPI.Win_lock(MPI.LOCK_EXCLUSIVE, dest, no_assert, win)\n",
    "MPI.Put([Float64(rank)], nb_elms, dest, offset, win)\n",
    "MPI.Win_unlock(dest, win)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Read the array at rank 0\n",
    "```julia\n",
    "if rank == dest\n",
    "  MPI.Win_lock(MPI.LOCK_SHARED, dest, no_assert, win)\n",
    "  println(\"My partners sent me this: \", shared')\n",
    "  MPI.Win_unlock(dest, win)\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Output:\n",
    "```text\n",
    "My partners sent me this: [0.0 1.0 2.0 3.0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application: MPIArrays\n",
    "\n",
    "* Distributed array built around a per-processor Julia array that is shared as an MPI Window\n",
    "* Implements `AbstractArray`\n",
    "* Simple implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How it works\n",
    "See `src/simplempiarray.jl` file:\n",
    "```julia\n",
    "struct SimpleMPIVector{T} <: AbstractVector{T}\n",
    "  localarray::Vector{T}\n",
    "  comm::MPI.Comm\n",
    "  win::MPI.Win\n",
    "\n",
    "  function SimpleMPIVector{T}(comm::MPI.Comm, len) where {T}\n",
    "    locarr = Vector{T}(undef, len)\n",
    "    win = MPI.Win()\n",
    "    MPI.Win_create(locarr, MPI.INFO_NULL, comm, win)\n",
    "    return new{T}(locarr, comm, win)\n",
    "  end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```julia\n",
    "Base.IndexStyle(::Type{SimpleMPIVector{T}}) where {T} = IndexLinear()\n",
    "Base.size(v::SimpleMPIVector) = size(v.localarray) .* MPI.Comm_size(v.comm)\n",
    "\n",
    "function Base.getindex(v::SimpleMPIVector{T}, i::Int) where {T}\n",
    "  loclen = length(v.localarray)\n",
    "  target_rank = (i-1) รท loclen\n",
    "  local_index = (i-1) % loclen\n",
    "  result = Ref{T}()\n",
    "  MPI.Win_lock(MPI.LOCK_SHARED, target_rank, 0, v.win)\n",
    "  MPI.Get(result, 1, target_rank, local_index, v.win)\n",
    "  MPI.Win_unlock(target_rank, v.win)\n",
    "  return result[]\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usage example:\n",
    "```julia\n",
    "vec = SimpleMPIVector{Int}(comm, mylength)\n",
    "rank == 0 && @show length(vec)\n",
    "\n",
    "for i in mystart:myend\n",
    "  vec[i] = rank\n",
    "end\n",
    "\n",
    "MPI.Barrier(comm)\n",
    "\n",
    "rank == 0 && println(\"The global vector is \", vec')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Output of `mpirun -np 4 julia --project src/simplempiarray.jl`\n",
    "```text\n",
    "length(vec) = 16\n",
    "The global vector is [0 0 0 0 1 1 1 1 2 2 2 2 3 3 3 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MPIArrays.jl\n",
    "\n",
    "* A more elaborate implementation of MPI-3 RMA based arrays can be found in [MPIArrays.jl](https://github.com/barche/MPIArrays.jl).\n",
    "* Matrix-vector product example:\n",
    "\n",
    "```julia\n",
    "# Create an uninitialized matrix and vector\n",
    "x = MPIArray{Float64}(N)\n",
    "A = MPIArray{Float64}(N,N)\n",
    "\n",
    "# Set local values\n",
    "forlocalpart!(m -> fill!(m,1+rank),x)\n",
    "forlocalpart!(m -> fill!(m,1+rank),A)\n",
    "sync(A, x) # Make sure initialization is done everywhere\n",
    "\n",
    "b = A*x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some MPIArrays features\n",
    "* Distribute automatically or manually choose per-process size\n",
    "* Initialize using existing local arrays\n",
    "* `filter`\n",
    "* Redistribute an array\n",
    "* Extract a regular block to a local matrix\n",
    "* `GhostedBlock` containing arbitrary off-processor entries\n",
    "* Very limited linear algebra (only mat-vec, to test performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Julia IO on top of MPI\n",
    "Implemented in MPI.jl [PR 203](https://github.com/JuliaParallel/MPI.jl/pull/203). Example:\n",
    "\n",
    "```julia\n",
    "# winio receives data on all processes from anyone\n",
    "const winio = WindowIO(comm)\n",
    "# everyone can write to process 0\n",
    "const writer = WindowWriter(winio, 0)\n",
    "\n",
    "println(writer, \"Hello from $rank\")\n",
    "flush(writer)\n",
    "\n",
    "if rank == 0\n",
    "  nb_received = 0\n",
    "  while nb_received < MPI.Comm_size(comm)\n",
    "    received = readline(winio)\n",
    "    println(\"Received message: $received\")\n",
    "    global nb_received += 1\n",
    "  end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Output of `mpirun -np 4 julia --project src/io.jl`:\n",
    "\n",
    "```text\n",
    "Received message: Hello from 0\n",
    "Received message: Hello from 2\n",
    "Received message: Hello from 1\n",
    "Received message: Hello from 3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implemented functions\n",
    "```julia\n",
    "Base.nb_available(w::WindowIO)\n",
    "Base.wait(w::WindowIO)\n",
    "Base.isopen(w::WindowIO)\n",
    "Base.eof(w::WindowIO)\n",
    "Base.iswritable(::WindowIO)\n",
    "Base.isreadable(::WindowIO)\n",
    "Base.close(w::WindowIO)\n",
    "Base.read(w::WindowIO, ::Type{UInt8})\n",
    "Base.read(w::WindowIO, nb::Integer; all::Bool=true)\n",
    "Base.readbytes!(w::WindowIO, b::AbstractVector{UInt8}, nb=length(b); all::Bool=true)\n",
    "Base.readavailable(w::WindowIO)\n",
    "Base.unsafe_read(w::WindowIO, p::Ptr{UInt8}, nb::UInt)\n",
    "Base.write(w::WindowWriter, b::UInt8)\n",
    "Base.unsafe_write(w::WindowWriter, p::Ptr{UInt8}, nb::UInt)\n",
    "Base.flush(s::WindowWriter)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Waiting for data\n",
    "```julia\n",
    "w.waiter = Task(function()\n",
    "            wait(w.read_requested)\n",
    "            while w.is_open\n",
    "                while !has_data_available(w) && w.is_open\n",
    "                    yield()\n",
    "                end\n",
    "                if w.is_open\n",
    "                    notify(w.data_available)\n",
    "                    wait(w.read_requested)\n",
    "                end\n",
    "            end\n",
    "        end)\n",
    "\n",
    "        yield(w.waiter)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustermanager\n",
    "* Manages communication for native Julia parallelism\n",
    "* Communication using `TCPSocket <: IO`\n",
    "* ClusterManagers based on MPI using `Send` and `Receive` existed\n",
    "* New option using `IO`\n",
    "* Intersperse MPI calls and native Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ClusterManager mode\n",
    "\n",
    "```julia\n",
    "using Distributed, MPI\n",
    "mgr = MPI.start_main_loop(MPI.MPI_WINDOW_IO)\n",
    "\n",
    "@everywhere begin\n",
    "  const comm = MPI.COMM_WORLD\n",
    "  const vec = fill(1,10)\n",
    "  s = MPI.Reduce(sum(vec), +, 0, comm)\n",
    "end\n",
    "\n",
    "println(\"Sum is $s\")\n",
    "\n",
    "@everywhere println(\"Hello from $(MPI.Comm_rank(comm))\")\n",
    "\n",
    "MPI.stop_main_loop(mgr)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Output of `mpirun -np 4 julia --project src/clustermanager.jl`:\n",
    "```text\n",
    "Sum is 40\n",
    "Hello from 0\n",
    "\tFrom worker 3:\tHello from 2\n",
    "\tFrom worker 4:\tHello from 3\n",
    "\tFrom worker 2:\tHello from 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MPI mode\n",
    "\n",
    "```julia\n",
    "using Distributed, MPI\n",
    "mgr = MPI.start_main_loop(MPI.MPI_WINDOW_NOWAIT)\n",
    "\n",
    "const comm = MPI.COMM_WORLD\n",
    "const vec = fill(1,10)\n",
    "s = MPI.Reduce(sum(vec), +, 0, comm)\n",
    "\n",
    "@cluster begin\n",
    "  println(\"Sum is $s\")\n",
    "  @everywhere println(\"Hello from $(MPI.Comm_rank(comm))\")\n",
    "end\n",
    "\n",
    "MPI.stop_main_loop(mgr)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some benchmarks\n",
    "Single threaded `A*x`:\n",
    "![Single threaded](figs/singlethread.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Some benchmarks\n",
    "Multi-threaded `A*x`:\n",
    "![Multi threaded](figs/multithread.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions\n",
    "* MPI and Julia parallelism can work together\n",
    "* Performance on-par with traditional codes\n",
    "* ClusterManagers allow flexible choice between the Julia and MPI way, even in the same program\n",
    "* My current use case: MPIArrays.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Thanks\n",
    "\n",
    "Many thanks to the [MPI.jl contributors](https://github.com/JuliaParallel/MPI.jl/graphs/contributors)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.7.0-beta2",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
