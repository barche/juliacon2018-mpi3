{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parallel computing with MPI-3 RMA and Julia\n",
    "\n",
    "<br/>\n",
    "### JuliaCon 2018\n",
    "<br/>\n",
    "[Bart Janssens](https://github.com/barche)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bird's eye view\n",
    "\n",
    "1. Classic MPI message passing\n",
    "2. Native Julia parallelism\n",
    "3. MPI-3 one-sided \"Remote Memory Access\" (RMA)\n",
    "4. Applications:\n",
    "    - MPI Arrays\n",
    "    - Julia IO on MPI\n",
    "    - New clustermanager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The MPI way\n",
    "* MPI: Message passing interface\n",
    "* Original: **cooperative** communication\n",
    "\n",
    "|Rank 0|Rank 1|\n",
    "|---|---|---|\n",
    "|`Init()`|`Init()`|\n",
    "|`mymessage = \"Hi 1!\"`||\n",
    "|`Send(mymessage,1)`|`Recv(buffer)`|\n",
    "|`Finalize()`|`Finalize()`|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The same program is sent to all ranks:\n",
    "```julia\n",
    "using MPI\n",
    "\n",
    "MPI.Init()\n",
    "\n",
    "const comm = MPI.COMM_WORLD\n",
    "const rank = MPI.Comm_rank(comm)\n",
    "const tag = 0\n",
    "\n",
    "if rank == 0\n",
    "  msg = unsafe_wrap(Vector{UInt8},\"Hi 1!\")\n",
    "  println(\"Sending message from rank 0 to rank 1\")\n",
    "  MPI.Send(msg, 1, tag, comm)\n",
    "elseif rank == 1\n",
    "  rec_buf = fill(UInt8('_'), 10)\n",
    "  MPI.Recv!(rec_buf, 0, tag, comm)\n",
    "  println(\"Received message $(String(rec_buf)) from rank 0\")\n",
    "end\n",
    "\n",
    "println(\"Rank $rank is finalizing.\")\n",
    "MPI.Finalize()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Output of `mpirun -np 4 julia --project src/sendrecv.jl`:\n",
    "\n",
    "```text\n",
    "Sending message from rank 0 to rank 1\n",
    "Rank 3 is finalizing.\n",
    "Rank 2 is finalizing.\n",
    "Received message Hi 1!_____ from rank 0\n",
    "Rank 0 is finalizing.\n",
    "Rank 1 is finalizing.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Collective calls\n",
    "* Send/Receive works only between two processes\n",
    "* Collective calls exist, e.g. summing an array:\n",
    "\n",
    "```julia\n",
    "using MPI\n",
    "\n",
    "MPI.Init()\n",
    "\n",
    "const comm = MPI.COMM_WORLD\n",
    "const rank = MPI.Comm_rank(comm)\n",
    "const root = 0\n",
    "\n",
    "vec = fill(1,10)\n",
    "s = MPI.Reduce(sum(vec), +, root, comm) # MUST be called on all ranks\n",
    "println(\"Sum on rank $rank: $(repr(s))\")\n",
    "\n",
    "MPI.Finalize()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Output of `mpirun -np 4 julia --project src/reduce.jl`:\n",
    "\n",
    "```text\n",
    "Sum on rank 2: nothing\n",
    "Sum on rank 3: nothing\n",
    "Sum on rank 0: 40\n",
    "Sum on rank 1: nothing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Or using `Allreduce`:\n",
    "```text\n",
    "Sum on rank 2: 40Sum on rank 3: 40\n",
    "Sum on rank 1: 40\n",
    "\n",
    "Sum on rank 0: 40\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Julia way\n",
    "* One-sided\n",
    "* Remote references and remote call\n",
    "* Directed by a \"master\" process.\n",
    "* \"Workers\" do the actual work and can be started and stopped using `addprocs` and `rmprocs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Starting up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 2\n",
       " 3\n",
       " 4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Distributed\n",
    "addprocs(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Creating some arrays remotely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Future,1}:\n",
       " Future(2, 1, 102, nothing)\n",
       " Future(3, 1, 103, nothing)\n",
       " Future(4, 1, 104, nothing)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remote_arrays = [@spawnat w fill(myid(),10) for w in workers()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Fetching the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Int64,1}:\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch(remote_arrays[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Compute the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 3:\tComputing sum for Future(3, 1, 103, nothing) on process 3\n",
      "      From worker 4:\tComputing sum for Future(4, 1, 104, nothing) on process 4\n",
      "      From worker 2:\tComputing sum for Future(2, 1, 102, Some([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])) on process 2\n"
     ]
    }
   ],
   "source": [
    "sums = map(remote_arrays) do x\n",
    "    @spawn begin\n",
    "        println(\"Computing sum for $x on process $(myid())\")\n",
    "        return sum(fetch(x))\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(fetch.(sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Future,1}:\n",
       " Future(2, 1, 106, Some(20))\n",
       " Future(3, 1, 107, Some(30))\n",
       " Future(4, 1, 108, Some(40))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or using the higher level `@distributed` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 2:\tComputing sum for Future(2, 1, 102, Some([2, 2, 2, 2, 2, 2, 2, 2, 2, 2])) on process 2\n",
      "      From worker 3:\tComputing sum for Future(3, 1, 103, Some([3, 3, 3, 3, 3, 3, 3, 3, 3, 3])) on process 3\n",
      "      From worker 4:\tComputing sum for Future(4, 1, 104, Some([4, 4, 4, 4, 4, 4, 4, 4, 4, 4])) on process 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@distributed (+) for x=remote_arrays\n",
    "    println(\"Computing sum for $x on process $(myid())\")\n",
    "    sum(fetch(x))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MPI one-sided communication\n",
    "* First introduced in MPI-2, extended in MPI-3\n",
    "* Focus here on \"Remote Memory Access\" (RMA)\n",
    "* Basic idea: define a region of memory on each process, that other processes can read from or write to\n",
    "* Target can be passive\n",
    "* Needs locking!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Setup of the shared memory region\n",
    "\n",
    "```julia\n",
    "# set up an array of length N on each process and mark it shared\n",
    "shared = zeros(N)\n",
    "win = MPI.Win()\n",
    "MPI.Win_create(shared, MPI.INFO_NULL, comm, win)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Have other ranks write the value of their rank to rank 0\n",
    "\n",
    "```julia\n",
    "# Every rank writes to the array held by rank 0 (dest) at its rank position\n",
    "offset = rank\n",
    "nb_elms = 1\n",
    "MPI.Win_lock(MPI.LOCK_EXCLUSIVE, dest, no_assert, win)\n",
    "MPI.Put([Float64(rank)], nb_elms, dest, offset, win)\n",
    "MPI.Win_unlock(dest, win)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Read the array at rank 0\n",
    "```julia\n",
    "if rank == dest\n",
    "  MPI.Win_lock(MPI.LOCK_SHARED, dest, no_assert, win)\n",
    "  println(\"My partners sent me this: \", shared')\n",
    "  MPI.Win_unlock(dest, win)\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Output:\n",
    "```text\n",
    "My partners sent me this: [0.0 1.0 2.0 3.0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application: MPIArrays\n",
    "\n",
    "* Distributed array built around a per-processor Julia array that is shared as an MPI Window\n",
    "* Implements `AbstractArray`\n",
    "* Simple implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How it works\n",
    "See `src/simplempiarray.jl` file:\n",
    "```julia\n",
    "struct SimpleMPIVector{T} <: AbstractVector{T}\n",
    "  localarray::Vector{T}\n",
    "  comm::MPI.Comm\n",
    "  win::MPI.Win\n",
    "  myrank::Int\n",
    "\n",
    "  function SimpleMPIVector{T}(comm::MPI.Comm, len) where {T}\n",
    "    locarr = Vector{T}(undef, len)\n",
    "    win = MPI.Win()\n",
    "    MPI.Win_create(locarr, MPI.INFO_NULL, comm, win)\n",
    "    return new{T}(locarr, comm, win, MPI.Comm_rank(comm))\n",
    "  end\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```julia\n",
    "Base.IndexStyle(::Type{SimpleMPIVector{T}}) where {T} = IndexLinear()\n",
    "Base.size(v::SimpleMPIVector) = size(v.localarray) .* MPI.Comm_size(v.comm)\n",
    "\n",
    "function Base.getindex(v::SimpleMPIVector{T}, i::Int) where {T}\n",
    "  loclen = length(v.localarray)\n",
    "  target_rank = (i-1) ÷ loclen\n",
    "  local_index = (i-1) % loclen\n",
    "  result = Ref{T}()\n",
    "  MPI.Win_lock(MPI.LOCK_SHARED, target_rank, 0, v.win)\n",
    "  MPI.Get(result, 1, target_rank, local_index, v.win)\n",
    "  MPI.Win_unlock(target_rank, v.win)\n",
    "  return result[]\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Usage example:\n",
    "```julia\n",
    "vec = SimpleMPIVector{Int}(comm, mylength)\n",
    "rank == 0 && @show length(vec)\n",
    "\n",
    "for i in mystart:myend\n",
    "  vec[i] = rank\n",
    "end\n",
    "\n",
    "MPI.Barrier(comm)\n",
    "\n",
    "rank == 0 && println(\"The global vector is \", vec')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Output of `mpirun -np 4 julia --project src/simplempiarray.jl`\n",
    "```text\n",
    "length(vec) = 16\n",
    "The global vector is [0 0 0 0 1 1 1 1 2 2 2 2 3 3 3 3]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "using DistributedArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Base.procs is deprecated: it has been moved to the standard library package `Distributed`.\n",
      "Add `using Distributed` to your imports.\n",
      "  likely near In[7]:1\n",
      "WARNING: Base.nprocs is deprecated: it has been moved to the standard library package `Distributed`.\n",
      "Add `using Distributed` to your imports.\n",
      "  likely near In[7]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100-element DArray{Float64,1,Array{Float64,1}}:\n",
       " 0.8531564517665009  \n",
       " 0.2089520384280661  \n",
       " 0.4861428012318145  \n",
       " 0.28061196776306474 \n",
       " 0.6246679083390314  \n",
       " 0.8919679135956722  \n",
       " 0.9971890730164081  \n",
       " 0.9376863917843443  \n",
       " 0.38369774043545846 \n",
       " 0.7200490732563147  \n",
       " 0.2960357911575635  \n",
       " 0.5553101074243334  \n",
       " 0.7756154549737408  \n",
       " ⋮                   \n",
       " 0.6253538163103969  \n",
       " 0.409447688103004   \n",
       " 0.17537936529799492 \n",
       " 0.4931933437740019  \n",
       " 0.5055103617375258  \n",
       " 0.4544441735749394  \n",
       " 0.6711289021422038  \n",
       " 0.019533977256011026\n",
       " 0.4761453639397015  \n",
       " 0.40448152051286246 \n",
       " 0.6382510233180076  \n",
       " 0.12140908196787525 "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Base.Distributed is deprecated, run `using Distributed` instead\n",
      "  likely near /Users/bjanssens/.julia/packages/IJulia/RBfP/src/kernel.jl:32\n",
      "WARNING: Base.Distributed is deprecated, run `using Distributed` instead\n",
      "  likely near /Users/bjanssens/.julia/packages/IJulia/RBfP/src/kernel.jl:32\n",
      "WARNING: Base.Distributed is deprecated, run `using Distributed` instead\n",
      "  likely near /Users/bjanssens/.julia/packages/IJulia/RBfP/src/kernel.jl:32\n"
     ]
    }
   ],
   "source": [
    "distribute(rand(100), procs=procs(), dist=(nprocs(),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.7.0-beta2",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
